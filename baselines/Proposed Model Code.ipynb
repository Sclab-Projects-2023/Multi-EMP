{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Layer, Add\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels (Train, Validation, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('train-data-annotation-v1.csv')['Empathic-Level']\n",
    "val_labels = pd.read_csv('val-data-annotation-v1.csv')['Empathic-Level']\n",
    "test_labels =pd.read_csv('test-data-annotation-v1.csv')['Empathic-Level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FaceNet\n",
    "train_visual_facenet = np.load('visual_train_features_facenet.npy')\n",
    "val_visual_facenet = np.load('visual_val_features_facenet.npy')\n",
    "test_visual_facenet = np.load('visual_test_features_facenet.npy')\n",
    "\n",
    "train_visual_facenet.shape, val_visual_facenet.shape, test_visual_facenet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FER\n",
    "train_visual_fer = np.load('train_visual_features_fer.npy')\n",
    "val_visual_fer = np.load('val_visual_features_fer.npy')\n",
    "test_visual_fer = np.load('test_visual_features_fer.npy')\n",
    "\n",
    "train_visual_fer.shape, val_visual_fer.shape, test_visual_fer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pose\n",
    "train_visual_pose = np.load('visual_train_features_pose.npy')\n",
    "val_visual_pose = np.load('visual_val_features_pose.npy')\n",
    "test_visual_pose = np.load('visual_test_features_pose.npy')\n",
    "\n",
    "train_visual_pose.shape, val_visual_pose.shape, test_visual_pose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaze\n",
    "train_visual_gaze = np.load('visual_train_features_gaze.npy')\n",
    "val_visual_gaze = np.load('visual_val_features_gaze.npy')\n",
    "test_visual_gaze = np.load('visual_test_features_gaze.npy')\n",
    "\n",
    "train_visual_gaze.shape, val_visual_gaze.shape, test_visual_gaze.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wav2Vec 2.0\n",
    "\n",
    "train_audio_features_wav = np.load('train_listener_features_wv2.npy')\n",
    "val_audio_features_wav = np.load('val_listener_features_wv2.npy')\n",
    "test_audio_features_wav = np.load('test_listener_features_wv2.npy')\n",
    "\n",
    "train_audio_features_wav.shape, val_audio_features_wav.shape, test_audio_features_wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuBERT\n",
    "\n",
    "train_audio_features_hb = np.load('train_listener_features_hb.npy')\n",
    "val_audio_features_hb = np.load('val_listener_features_hb.npy')\n",
    "test_audio_features_hb = np.load('test_listener_features_hb.npy')\n",
    "\n",
    "train_audio_features_hb.shape, val_audio_features_hb.shape, test_audio_features_hb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_features = np.load('train_text_features_dikobert.npy')\n",
    "val_text_features = np.load('val_text_features_dikobert.npy')\n",
    "test_text_features = np.load('test_text_features_dikobert.npy')\n",
    "\n",
    "train_text_features.shape, val_text_features.shape, test_text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the bio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bio_features = np.load('./extracted-features/train_listener_bio_features.npy')\n",
    "val_bio_features = np.load('./extracted-features/val_listener_bio_features.npy')\n",
    "test_bio_features = np.load('./extracted-features/test_listener_bio_features.npy')\n",
    "\n",
    "train_bio_features.shape, val_bio_features.shape, test_bio_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # First dense layer with GELU activation\n",
    "        self.linear_1 = Dense(intermediate_size, activation='gelu')\n",
    "        # Second dense layer to project back to hidden_size\n",
    "        self.linear_2 = Dense(hidden_size)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply the feed-forward network\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(Layer):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout_rate):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        # Layer normalization before attention and feed-forward\n",
    "        self.layer_norm_1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_2 = LayerNormalization(epsilon=1e-6)\n",
    "        # Multi-head attention layer\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size)\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(hidden_size, intermediate_size, dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply layer normalization and self-attention\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attn_output = self.attention(hidden_state, hidden_state, hidden_state)\n",
    "        # Add residual connection\n",
    "        x = x + attn_output\n",
    "        # Apply layer normalization and feed-forward network\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_position_embeddings, feature_dim, dropout_rate=0.1):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            name=\"pos_embeddings\",\n",
    "            shape=[max_position_embeddings, feature_dim],\n",
    "            initializer=\"glorot_uniform\")\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get the sequence length and batch size from the input shape\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Generate position indices and gather the relevant embeddings\n",
    "        position_ids = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = tf.gather(self.position_embeddings, position_ids)\n",
    "        \n",
    "        # Broadcast the position embeddings to match the input shape\n",
    "        position_embeddings = tf.broadcast_to(position_embeddings, [batch_size, seq_length, tf.shape(x)[-1]])\n",
    "        \n",
    "        # Add the position embeddings to the input\n",
    "        embeddings = x + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.positional_embedding = PositionalEmbedding(max_position_embeddings, hidden_size, dropout_rate)\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        self.encoder_layers = [TransformerEncoderLayer(hidden_size, intermediate_size, num_heads, dropout_rate) \n",
    "                               for _ in range(num_hidden_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.positional_embedding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional MLP-Mixer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerLayer(Layer):\n",
    "    def __init__(self, tokens_mlp_dim, channels_mlp_dim, dropout_rate, return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.tokens_mlp_dim = tokens_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dense1 = Dense(self.tokens_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(input_shape[1], activation=tf.nn.gelu)\n",
    "        self.dense3 = Dense(self.channels_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense4 = Dense(input_shape[2], activation=tf.nn.gelu)\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Token mixing\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x_t = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x_t = self.dense1(x_t)\n",
    "        x_t = self.dense2(x_t)\n",
    "        x_t = tf.transpose(x_t, perm=[0, 2, 1])\n",
    "        x = Add()([x, x_t])\n",
    "\n",
    "        # Channel mixing\n",
    "        y = self.layer_norm2(x)\n",
    "        y = self.dense3(y)\n",
    "        y = self.dense4(y)\n",
    "        y = Add()([x, y])\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            y = tf.reduce_mean(y, axis=1)\n",
    "\n",
    "        return y\n",
    "\n",
    "class BackwardMixerLayer(Layer):\n",
    "    def __init__(self, tokens_mlp_dim, channels_mlp_dim, dropout_rate, return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.tokens_mlp_dim = tokens_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dense1 = Dense(self.tokens_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(input_shape[1], activation=tf.nn.gelu)\n",
    "        self.dense3 = Dense(self.channels_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense4 = Dense(input_shape[2], activation=tf.nn.gelu)\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Token mixing (backward)\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x_t = tf.reverse(tf.transpose(x, perm=[0, 2, 1]), axis=[1])\n",
    "        x_t = self.dense1(x_t)\n",
    "        x_t = self.dense2(x_t)\n",
    "        x_t = tf.reverse(tf.transpose(x_t, perm=[0, 2, 1]), axis=[1])\n",
    "        x = Add()([x, x_t])\n",
    "\n",
    "        # Channel mixing (backward)\n",
    "        y = self.layer_norm2(x)\n",
    "        y = tf.reverse(self.dense3(y), axis=[1])\n",
    "        y = self.dense4(y)\n",
    "        y = tf.reverse(y, axis=[1])\n",
    "        y = Add()([x, y])\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            y = tf.reduce_mean(y, axis=1)\n",
    "\n",
    "        return y\n",
    "\n",
    "class BidirectionalMixerLayer(Layer):\n",
    "    def __init__(self, tokens_mlp_dim, channels_mlp_dim, dropout_rate, merge_mode='sum', return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.forward_layer = MixerLayer(tokens_mlp_dim, channels_mlp_dim, dropout_rate, return_sequences)\n",
    "        self.backward_layer = BackwardMixerLayer(tokens_mlp_dim, channels_mlp_dim, dropout_rate, return_sequences)\n",
    "        self.merge_mode = merge_mode\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        forward_output = self.forward_layer(inputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        backward_output = self.backward_layer(inputs)\n",
    "        \n",
    "        # Merge outputs\n",
    "        if self.merge_mode == 'concat':\n",
    "            combined_output = tf.concat([forward_output, backward_output], axis=-1)\n",
    "        elif self.merge_mode == 'sum':\n",
    "            combined_output = forward_output + backward_output\n",
    "        elif self.merge_mode == 'mul':\n",
    "            combined_output = forward_output * backward_output\n",
    "        elif self.merge_mode == 'ave':\n",
    "            combined_output = (forward_output + backward_output) / 2\n",
    "        else:\n",
    "            combined_output = [forward_output, backward_output]\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition (Visual, Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_input_shape_facenet = (train_visual_facenet.shape[1], train_visual_facenet.shape[2])\n",
    "visual_input_shape_fer = (train_visual_fer.shape[1], train_visual_fer.shape[2])\n",
    "visual_input_shape_pose = (train_visual_pose.shape[1], train_visual_pose.shape[2])\n",
    "visual_input_shape_gaze = (train_visual_gaze.shape[1], train_visual_gaze.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input_shape_wav = (train_audio_features_wav.shape[1], train_audio_features_wav.shape[2])\n",
    "audio_input_shape_hb = (train_audio_features_hb.shape[1], train_audio_features_hb.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model_facenet = Sequential([\n",
    "    Input(shape=visual_input_shape_facenet),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])\n",
    "\n",
    "visual_model_fer = Sequential([\n",
    "    Input(shape=visual_input_shape_fer),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])\n",
    "\n",
    "visual_model_pose = Sequential([\n",
    "    Input(shape=visual_input_shape_pose),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])\n",
    "\n",
    "visual_model_gaze = Sequential([\n",
    "    Input(shape=visual_input_shape_gaze),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model_wav = Sequential([\n",
    "    Input(shape=audio_input_shape_wav),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])\n",
    "\n",
    "audio_model_hb = Sequential([\n",
    "    Input(shape=audio_input_shape_hb),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    BidirectionalMixerLayer(tokens_mlp_dim=128, channels_mlp_dim=128, dropout_rate=0.4, return_sequences=True),\n",
    "    GlobalAveragePooling1D()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForText(tf.keras.Model):\n",
    "    def __init__(self, num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings, num_classes):\n",
    "        super(TransformerForText, self).__init__()\n",
    "        self.encoder = TransformerEncoder(num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings)\n",
    "        self.pooling = GlobalAveragePooling1D()\n",
    "        self.dense = Dense(256)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)  \n",
    "        x = self.pooling(x)  \n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "num_hidden_layers = 4\n",
    "hidden_size = 768  \n",
    "intermediate_size = 2048  \n",
    "num_heads = 6  \n",
    "dropout_rate = 0.1\n",
    "max_position_embeddings = 210\n",
    "num_classes = 7\n",
    "feature_dim = hidden_size\n",
    "\n",
    "text_model = TransformerForText(num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForBio(tf.keras.Model):\n",
    "    def __init__(self, num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings):\n",
    "        super(TransformerForBio, self).__init__()\n",
    "        self.encoder = TransformerEncoder(num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings)\n",
    "        self.pooling = GlobalAveragePooling1D()\n",
    "        self.dense = Dense(256)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "num_hidden_layers = 4\n",
    "hidden_size = 4 \n",
    "intermediate_size = 32  \n",
    "num_heads = 2 \n",
    "dropout_rate = 0.1\n",
    "max_position_embeddings = 3\n",
    "\n",
    "bio_model = TransformerForBio(num_hidden_layers, hidden_size, intermediate_size, num_heads, dropout_rate, max_position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.units = units\n",
    "        # Dense layers for transforming query, key, and value\n",
    "        self.dense_query = Dense(units)\n",
    "        self.dense_key = Dense(units)\n",
    "        self.dense_value = Dense(units)\n",
    "        # Scaling factor for dot product attention\n",
    "        self.scale = tf.sqrt(tf.cast(units, tf.float32))\n",
    "        # Softmax layer for attention weights\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        query, key = inputs\n",
    "        # Transform query, key, and value\n",
    "        query = self.dense_query(query)\n",
    "        key = self.dense_key(key)\n",
    "        value = self.dense_value(key)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        score = tf.matmul(query, key, transpose_b=True) / self.scale\n",
    "        # Apply softmax to get attention weights\n",
    "        alignment = self.softmax(score)\n",
    "        # Compute the context vector\n",
    "        context = tf.matmul(alignment, value)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(WeightedSum, self).__init__()\n",
    "        # Initialize trainable weights for attention\n",
    "        self.attention_weights = self.add_weight(shape=(num_inputs,),\n",
    "                                                 initializer='random_normal',\n",
    "                                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Convert all inputs to float32 tensors\n",
    "        tensor_inputs = [tf.convert_to_tensor(input, dtype=tf.float32) for input in inputs]\n",
    "        \n",
    "        # Stack inputs along the last axis\n",
    "        stacked = tf.stack(tensor_inputs, axis=-1)\n",
    "        \n",
    "        # Apply softmax to attention weights and compute weighted sum\n",
    "        weighted_sum = tf.reduce_sum(stacked * tf.nn.softmax(self.attention_weights), axis=-1)\n",
    "        \n",
    "        return weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-modal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(tf.keras.Model):\n",
    "    def __init__(self, visual_model_facenet, visual_model_fer, visual_model_pose, visual_model_gaze, \n",
    "                 audio_model_wav, audio_model_hb, text_model, bio_model):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        # Initialize individual models for each modality\n",
    "        self.visual_model_facenet = visual_model_facenet\n",
    "        self.visual_model_fer = visual_model_fer\n",
    "        self.visual_model_pose = visual_model_pose\n",
    "        self.visual_model_gaze = visual_model_gaze\n",
    "        self.audio_model_wav = audio_model_wav\n",
    "        self.audio_model_hb = audio_model_hb\n",
    "        self.text_model = text_model\n",
    "        self.bio_model = bio_model  \n",
    "        \n",
    "        # Cross-attention layers for visual and audio modalities\n",
    "        self.cross_attention_visual_1 = CrossAttention(256)\n",
    "        self.cross_attention_visual_2 = CrossAttention(256)\n",
    "        self.cross_attention_audio = CrossAttention(256)\n",
    "        \n",
    "        # Weighted sum layer for fusion of all modalities\n",
    "        self.weighted_sum = WeightedSum(num_inputs=5)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = Dense(7, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs for each modality\n",
    "        visual_input_facenet, visual_input_fer, visual_input_pose, visual_input_gaze, \\\n",
    "        audio_input_wav, audio_input_hb, text_input, bio_input = inputs\n",
    "        \n",
    "        # Process inputs through respective models\n",
    "        visual_output_facenet = self.visual_model_facenet(visual_input_facenet)\n",
    "        visual_output_fer = self.visual_model_fer(visual_input_fer)\n",
    "        visual_output_pose = self.visual_model_pose(visual_input_pose)\n",
    "        visual_output_gaze = self.visual_model_gaze(visual_input_gaze)\n",
    "        audio_output_wav = self.audio_model_wav(audio_input_wav)\n",
    "        audio_output_hb = self.audio_model_hb(audio_input_hb)\n",
    "        text_output = self.text_model(text_input)\n",
    "        bio_output = self.bio_model(bio_input)  \n",
    "        \n",
    "        # Apply cross-attention to visual outputs\n",
    "        cross_attention_visual_output_1 = self.cross_attention_visual_1([visual_output_facenet, visual_output_fer])\n",
    "        cross_attention_visual_output_2 = self.cross_attention_visual_2([visual_output_pose, visual_output_gaze])\n",
    "    \n",
    "        # Apply cross-attention to audio outputs\n",
    "        cross_attention_audio_output = self.cross_attention_audio([audio_output_wav, audio_output_hb])\n",
    "    \n",
    "        # Fuse all modalities using weighted sum\n",
    "        fusion = self.weighted_sum([\n",
    "            cross_attention_visual_output_1, \n",
    "            cross_attention_visual_output_2, \n",
    "            cross_attention_audio_output, \n",
    "            text_output, \n",
    "            bio_output\n",
    "        ])\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.classifier(fusion)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultiModalModel(visual_model_facenet, visual_model_fer, visual_model_pose, visual_model_gaze, audio_model_wav, audio_model_hb, text_model, bio_model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized F1-Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, patience=300, checkpoint_filepath='file_path', batch_size=2):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_weights = None\n",
    "        self.best_f1 = -np.inf\n",
    "        self.wait = 0\n",
    "        self.checkpoint_filepath = checkpoint_filepath\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if not os.path.exists(self.checkpoint_filepath):\n",
    "            os.makedirs(self.checkpoint_filepath)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_predict = np.argmax(self.model.predict(self.validation_data[0], batch_size=self.batch_size), axis=1)\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = classification_report(val_targ, val_predict, output_dict=True)['weighted avg']['f1-score']\n",
    "        logs['val_f1'] = _val_f1\n",
    "\n",
    "        if _val_f1 > self.best_f1:\n",
    "            self.best_f1 = _val_f1\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.model.save_weights(f\"{self.checkpoint_filepath}epoch_{epoch:02d}_val_f1_{_val_f1:.5f}.h5\")\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "    \n",
    "    learning_rate = 2e-5\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    f1_score_callback = F1ScoreCallback(validation_data=((val_visual_facenet, val_visual_fer, val_visual_pose, val_visual_gaze, \n",
    "                                                          val_audio_features_wav, val_audio_features_hb, val_text_features, val_bio_features), val_labels))\n",
    "\n",
    "    model.fit((train_visual_facenet, train_visual_fer, train_visual_pose, train_visual_gaze, train_audio_features_wav, \n",
    "               train_audio_features_hb, train_text_features, train_bio_features), train_labels, epochs=1000, \n",
    "              validation_data=((val_visual_facenet, val_visual_fer, val_visual_pose, val_visual_gaze, \n",
    "                                val_audio_features_wav, val_audio_features_hb, val_text_features, val_bio_features), val_labels), \n",
    "              batch_size=2)\n",
    "\n",
    "train_and_evaluate(baseline_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dep-new",
   "language": "python",
   "name": "ai_dep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
