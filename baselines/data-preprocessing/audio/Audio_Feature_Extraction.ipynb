{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('train-data-annotation-v1.csv')\n",
    "\n",
    "# Load feature extractor and model\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "\n",
    "# Set up resampler\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        speech, * = torchaudio.load(file_path)\n",
    "        speech = resampler(speech)\n",
    "        \n",
    "        if speech.shape[0] == 2:\n",
    "            speech = speech.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        speech = speech.squeeze()\n",
    "        \n",
    "        inputs = feature_extractor(speech, return_tensors=\"pt\", padding=True, sampling_rate=16000)     \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        return outputs.logits.squeeze().cpu().numpy()\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def adjust_features(features, target_len):\n",
    "    adjusted_features = []\n",
    "    for feat in features:\n",
    "        if feat is not None:\n",
    "            if feat.shape[0] < target_len:\n",
    "                adjusted_feat = np.pad(feat, ((0, target_len - feat.shape[0]), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                adjusted_feat = feat[:target_len, :]\n",
    "            adjusted_features.append(adjusted_feat)\n",
    "        else:\n",
    "            adjusted_features.append(np.zeros((target_len, 32)))  # 32 is the dimension of the feature vector\n",
    "    return np.array(adjusted_features)\n",
    "\n",
    "def extract_features_for_role(vid_names, role):\n",
    "    features = []\n",
    "    for vid in tqdm(vid_names, desc=f\"Extracting {role} features\"):\n",
    "        file_path = f\"audio-files-split-2/{vid}/{vid}_{role}.wav\"\n",
    "        feature = extract_features(file_path)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # Calculate median length of feature vectors\n",
    "    lengths = [feat.shape[0] for feat in features if feat is not None]\n",
    "    median_len = int(np.median(lengths))\n",
    "    \n",
    "    print(f\"Median length for {role}: {median_len}\")\n",
    "    \n",
    "    # Adjust to median length\n",
    "    adjusted_features = adjust_features(features, median_len)\n",
    "    \n",
    "    return adjusted_features\n",
    "\n",
    "# Extract features according to the order of 'VID_NAME' in the CSV file\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "listener_features = extract_features_for_role(vid_names, 'listener')\n",
    "\n",
    "print(\"Talker features shape:\", talker_features.shape)\n",
    "print(\"Listener features shape:\", listener_features.shape)\n",
    "print(\"Features saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('val-data-annotation-v1.csv')\n",
    "\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "\n",
    "val_talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "val_listener_features = extract_features_for_role(vid_names, 'listener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test-data-annotation-v1.csv')\n",
    "\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "\n",
    "test_talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "test_listener_features = extract_features_for_role(vid_names, 'listener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timesteps = min(listener_features.shape[1], val_listener_features.shape[1], test_listener_features.shape[1])\n",
    "\n",
    "train_features_l = listener_features[:, :min_timesteps, :]\n",
    "val_features_l = val_listener_features[:, :min_timesteps, :]\n",
    "test_features_l = test_listener_features[:, :min_timesteps, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wav2Vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('train-data-annotation-v1.csv')\n",
    "\n",
    "# Load Wav2Vec 2.0 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Set up resampler\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        speech, sr = torchaudio.load(file_path)\n",
    "        speech = resampler(speech)\n",
    "        \n",
    "        if speech.shape[0] == 2:\n",
    "            speech = speech.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        speech = speech.squeeze().numpy()\n",
    "        \n",
    "        inputs = processor(speech, return_tensors=\"pt\", padding=True, sampling_rate=16000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Use the last hidden state\n",
    "        features = outputs.last_hidden_state.squeeze().cpu().numpy()\n",
    "        \n",
    "        return features\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def adjust_features(features, target_len):\n",
    "    adjusted_features = []\n",
    "    for feat in features:\n",
    "        if feat is not None:\n",
    "            if feat.shape[0] < target_len:\n",
    "                adjusted_feat = np.pad(feat, ((0, target_len - feat.shape[0]), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                adjusted_feat = feat[:target_len, :]\n",
    "            adjusted_features.append(adjusted_feat)\n",
    "        else:\n",
    "            adjusted_features.append(np.zeros((target_len, 768)))  # Wav2Vec 2.0's feature vector dimension is 768\n",
    "    return np.array(adjusted_features)\n",
    "\n",
    "def extract_features_for_role(vid_names, role):\n",
    "    features = []\n",
    "    for vid in tqdm(vid_names, desc=f\"Extracting {role} features\"):\n",
    "        file_path = f\"audio-files-split-2/{vid}/{vid}_{role}.wav\"\n",
    "        feature = extract_features(file_path)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # Calculate median length of feature vectors\n",
    "    lengths = [feat.shape[0] for feat in features if feat is not None]\n",
    "    median_len = int(np.median(lengths))\n",
    "    \n",
    "    print(f\"Median length for {role}: {median_len}\")\n",
    "    \n",
    "    # Adjust to median length\n",
    "    adjusted_features = adjust_features(features, median_len)\n",
    "    \n",
    "    return adjusted_features\n",
    "\n",
    "# Extract features according to the order of 'VID_NAME' in the CSV file\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "listener_features = extract_features_for_role(vid_names, 'listener')\n",
    "\n",
    "print(\"Talker features shape:\", talker_features.shape)\n",
    "print(\"Listener features shape:\", listener_features.shape)\n",
    "print(\"Features extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('val-data-annotation-v1.csv')\n",
    "\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "\n",
    "val_talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "val_listener_features = extract_features_for_role(vid_names, 'listener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test-data-annotation-v1.csv')\n",
    "\n",
    "vid_names = df['VID_NAME'].tolist()\n",
    "\n",
    "test_talker_features = extract_features_for_role(vid_names, 'talker')\n",
    "test_listener_features = extract_features_for_role(vid_names, 'listener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timesteps = min(listener_features.shape[1], val_listener_features.shape[1], test_listener_features.shape[1])\n",
    "\n",
    "train_features_l = listener_features[:, :min_timesteps, :]\n",
    "val_features_l = val_listener_features[:, :min_timesteps, :]\n",
    "test_features_l = test_listener_features[:, :min_timesteps, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dep-new",
   "language": "python",
   "name": "ai_dep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
